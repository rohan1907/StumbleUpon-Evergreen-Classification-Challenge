# -*- coding: utf-8 -*-
"""evergreen classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19KSegi3SK16WxNcNATWaGhZKrC6XHYte

# **Stummble upon evergreen Classification Challenge**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 
import tensorflow_hub as hub
import matplotlib.pyplot as plt 
import seaborn as sns 
# %matplotlib inline 
import tensorflow as tf

# taking all columns of training set only for data exploration
df_train=pd.read_csv('/content/data/train.tsv',sep='\t')
# taking boilerplate column as an input for the model beacuse only this column contain lot of high quality text data
df_test=pd.read_csv('/content/data/test.tsv',sep='\t', usecols=['urlid','boilerplate'])

df_train.head()

#cleaning the train dataframe

df_train['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_train['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)

df_train['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_train['boilerplate']=df_train['boilerplate'].str.lower()

#Cleaning the test dataframe 

df_test['boilerplate'].replace(to_replace=r'"title":', value="",inplace=True,regex=True)
df_test['boilerplate'].replace(to_replace=r'"url":',value="",inplace=True,regex=True)

df_test['boilerplate'].replace(to_replace=r'{|}',value="",inplace=True,regex=True)
df_test['boilerplate']=df_test['boilerplate'].str.lower()

plt.figure(figsize=(15,10))
sns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);
plt.xlabel('Category');
plt.xticks(rotation=90);

text_train = df_train.boilerplate #extracting the train text from the bolierplate column
text_test = df_test.boilerplate #extracting the test text from the boilerplate column
urlid_test = df_test.urlid #extracting urlid from test dataset
y_train = df_train.label.values #extracting prediction labels

#splitting the train data into train and validation dataset
x_val = text_train[:3000]
partial_x_train = text_train[3000:]

y_val = y_train[:3000]
partial_y_train = y_train[3000:]
text_train[:3]

#downloading the word embedding model from tensorflow hub. 
model = "https://tfhub.dev/google/nnlm-en-dim50/2"
hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)

#creating the model 

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(64,activation='relu'))
model.add(tf.keras.layers.Dense(1,activation='sigmoid'))

model.summary()

#model compilation with adam optimizer 
model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer='adam',metrics=[tf.keras.metrics.AUC()])

#training the model, epochs = 40
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=40,
                    batch_size=512,
                    validation_data=(x_val, y_val),
                    verbose=1)

predictions=model.predict(text_test) #model prediction

val=[1 if i>=0.5 else 0 for i in predictions]

df_test['label']=val

df_test.to_csv('submissions.csv',columns=['urlid','label'],index=False) #saving submission file

history_dict = history.history
key=history_dict.keys()
key

acc = history_dict['auc']
val_acc = history_dict['val_auc']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()